# requires marmap
library(marmap)
myPacific <- getNOAA.bathy(-140, -60, -60, 65, resolution = 10, keep = TRUE, antimeridian = TRUE)
# load downloaded data
myPacific <- read.bathy("marmap_coord_-60;-60;120;65_res_10_anti.csv", header=TRUE)
# load downloaded data
myPacific <- read.bathy("marmap_coord_-140;-60;120;65_res_10_anti.csv", header=TRUE)
myPacific <- getNOAA.bathy(-140, -60, -60, 65, resolution = 10, keep = TRUE, antimeridian = TRUE)
# load downloaded data
myPacific <- read.bathy("marmap_coord_-140;-60;120;65_res_10_anti.csv", header=TRUE)
rm "myPcific"
myPacific1 <- getNOAA.bathy(-140, -60, -60, 65, resolution = 10, keep = TRUE, antimeridian = TRUE)
# load downloaded data
myPacific1 <- read.bathy("marmap_coord_-140;-60;120;65_res_10_anti.csv", header=TRUE)
# load downloaded data
myPacific1 <- read.bathy("marmap_coord_-140;-60;120;65_res_10_anti.csv", header=TRUE)
# blues palette for water, from shallowest to deepest
#Setting colour gradients for dif depths
blues <- c("lightsteelblue4", "lightsteelblue3", "lightsteelblue2", "lightsteelblue1")
myMin <- min(myPacific)
myMax <- max(myPacific)
#File format:
filename_pdf <- "Nereis_piscesae_V00.pdf"
#Defining your PDF:
pdf(file = filename_pdf, paper = "special", width = 10.5, height = 8, family = "Helvetica")
##Plotting map##
plot(myPacific, image = TRUE, land = TRUE, axes = FALSE, deep = 0, shallow = 0, step = 0, lwd = 0.3, bpal = list(c(0, myMax, "grey"), c(myMin, 0, blues)), xlab = "", ylab = "")
# left (latitude) axis line extends from -65 to 65
axis(side = 2, labels = FALSE, pos = 120, lwd.ticks = 0, yaxp = c(-60, 65, 1))
# left axis labels: -60 to 60 in increments of 20
axis(side = 2, pos = 120, at = c(-60, -40, -20, 0, 20, 40, 60))
# manually add axis labels for proper positioning relative to plot edge (not page edge)
title(ylab = "Latitude", line = 2)
title(xlab = "Longitude", line = 3)
# bottom (longitude) axis: 120 to 180, plus "180 to -60" plotted as 180 to 300
# cross left axis at minimum value
axis(side = 1, at = c(120, 140, 160, 180, 200, 220, 240, 260, 280, 300), labels = c("120", "140", "160", "180", "-160", "-140", "-120", "-100", "-80", "-60"), pos = -60)
# draw equator as thin dotted line
axis(side = 1, labels = FALSE, pos = 0, lty=3, lwd = 0.4, lwd.ticks = 0, xaxp = c(120, 300, 1))
# top axis is a line, no tick marks
axis(side = 3, labels = FALSE, pos = 65, lwd.ticks = 0, xaxp = c(120, 300, 1))
# right axis is a line with no tick marks
axis(side = 4, labels = FALSE, pos = 300, lwd.ticks = 0, yaxp = c(-60, 65, 1))
#Plotting points (Long, Lat)
points(360-130.0267, 45.9892, pch=24, bg="blue") #JDF
points(360-125.097, 44.67, pch=24, bg="red") #Hydrate_Ridge
points(360-122.0828, 36.7714, pch=24, bg="yellow") #Monterey Bay
points(360-117.7819, 32.9047, pch=24, bg="orange") #DelMar
points(360-111.4102, 27.0158, pch=24, bg="green") #Guyamas
# try legend
legendtext <- c("JDF", "Oregon Hydrate Ridge", "Monterey Bay", "Del Mar Seeps", "Guyamas Basin")
legend(124, -30, legendtext, pch = 25, pt.bg = c("blue", "red", "yellow", "orange", "green"), bg = "gray95")
# stop writing file/closing file
dev.off()
# requires marmap
library(marmap)
myPacific1 <- getNOAA.bathy(-140, -60, -60, 65, resolution = 10, keep = TRUE, antimeridian = TRUE)
# load downloaded data
myPacific1 <- read.bathy("marmap_coord_-140;-60;120;65_res_10_anti.csv", header=TRUE)
myPacific1 <- getNOAA.bathy(-140, -60, -60, 65, resolution = 10, keep = TRUE, antimeridian = TRUE)
# load downloaded data
myPacific1 <- read.bathy("marmap_coord_-140;-60;60;65_res_10_anti.csv", header=TRUE)
install.packages("seqinr")
install.packages("BiocManager")
}
.rs.restartR()
install.packages("seqinr", dependencies=TRUE)  # Will also install dependencies
if (!requireNamespace("BiocManager", quietly = TRUE)) {
install.packages("BiocManager")
}
BiocManager::install("msa")
library(seqinr)
library(msa)
install.packages("seqinr")
library(seqinr)
library(msa)
# Set working directory (update as needed)
setwd("~/Desktop/Vibrio_Project/Jan 2025/Keggs/aligned_files")
# List all aligned FASTA files
fasta_files <- list.files(pattern = "_aligned.fasta$")
# List all aligned FASTA files
fasta_files <- list.files(pattern = "_aligned.fasta$")
# Function to compute pairwise sequence identity
pairwise_identity <- function(alignment) {
n <- length(alignment)
identity_matrix <- matrix(0, n, n)
for (i in 1:n) {
for (j in i:n) {
matches <- sum(unlist(strsplit(alignment[i], "")) == unlist(strsplit(alignment[j], "")))
identity <- matches / nchar(alignment[i])
identity_matrix[i, j] <- identity
identity_matrix[j, i] <- identity
}
}
avg_identity <- mean(identity_matrix)
return(round(avg_identity * 100, 2))  # Convert to percentage
}
# Function to compute alignment entropy
alignment_entropy <- function(alignment) {
entropy_scores <- apply(consensusMatrix(alignment), 2, function(column) {
probs <- column / sum(column)
entropy <- -sum(probs * log2(probs + 1e-10))  # Avoid log(0)
return(entropy)
})
avg_entropy <- mean(entropy_scores)
return(avg_entropy)
}
# Analyze each aligned FASTA file
msa_results <- data.frame(File = character(), Avg_Identity = numeric(), Avg_Entropy = numeric(), stringsAsFactors = FALSE)
for (file in fasta_files) {
cat("Processing:", file, "\n")
# Read aligned sequences
alignment <- read.alignment(file, format = "fasta")
# Compute quality metrics
avg_identity <- pairwise_identity(alignment$seq)
msa_data <- msaRead(file, format = "fasta")
avg_entropy <- alignment_entropy(msa_data)
# Save results
msa_results <- rbind(msa_results, data.frame(File = file, Avg_Identity = avg_identity, Avg_Entropy = avg_entropy))
}
# Read existing summary stats
summary_stats <- read.csv("summary_stats.csv", stringsAsFactors = FALSE)
# Merge MSA results with summary stats
final_summary <- left_join(summary_stats, msa_results, by = "File")
# Save final dataset
write.csv(final_summary, "final_summary_stats.csv", row.names = FALSE)
cat("✅ MSA quality analysis and summary stats merged! Results saved to 'final_summary_stats.csv'\n")
# Clear workspace
rm(list=ls())
# Load necessary libraries
library(RecordLinkage)
library(Biostrings)
library(reshape2)
# Set working directory to where aligned files are stored
setwd("/Users/eesharangani/Desktop/Vibrio_Project/Jan 2025/Keggs/aligned_files")  # Update path if needed
# List all aligned FASTA files
fasta_files <- list.files(pattern = "_aligned.fasta$")
# Function to compute percentage identity matrix
percentage_identity_matrix <- function(seqs) {
n <- length(seqs)
identity_matrix <- matrix(0, n, n)
for (i in 1:n) {
for (j in i:n) {
matches <- sum(unlist(strsplit(seqs[i], "")) == unlist(strsplit(seqs[j], "")))
identity <- matches / nchar(seqs[i])  # Assumes all sequences are the same length
identity_matrix[i, j] <- identity
identity_matrix[j, i] <- identity  # Symmetric matrix
}
}
return(identity_matrix)
}
# Function to compute alignment entropy
alignment_entropy <- function(sequences) {
consensus_mat <- consensusMatrix(DNAStringSet(sequences))
entropy_scores <- apply(consensus_mat, 2, function(column) {
probs <- column / sum(column)  # Convert column to probabilities
entropy <- -sum(probs * log2(probs + 1e-10))  # Avoid log(0)
return(entropy)
})
avg_entropy <- mean(entropy_scores)
return(avg_entropy)
}
# Initialize results dataframe
results <- data.frame(File = character(), Avg_Identity = numeric(), Var_Identity = numeric(), Avg_Entropy = numeric(), stringsAsFactors = FALSE)
# Process each aligned FASTA file
for (file in fasta_files) {
cat("Processing:", file, "\n")  # Print progress
# Read aligned sequences
fafsa_data <- readDNAStringSet(file)
sequences <- as.character(fafsa_data)
# Compute identity matrix
identity_matrix <- percentage_identity_matrix(sequences)
# Compute summary statistics
avg_identity <- mean(identity_matrix)
var_identity <- var(as.vector(identity_matrix))
avg_entropy <- alignment_entropy(sequences)  # Compute entropy
# Save results
results <- rbind(results, data.frame(File = file, Avg_Identity = round(avg_identity * 100, 2),
Var_Identity = round(var_identity, 4), Avg_Entropy = round(avg_entropy, 4)))
}
View(results)
# Process each aligned FASTA file
for (file in fasta_files) {
cat("Processing:", file, "\n")  # Print progress
# Read aligned sequences
fafsa_data <- readDNAStringSet(file)
sequences <- as.character(fafsa_data)
# Compute identity matrix
identity_matrix <- percentage_identity_matrix(sequences)
# Compute summary statistics
avg_identity <- mean(identity_matrix)
var_identity <- var(as.vector(identity_matrix))
avg_entropy <- alignment_entropy(sequences)  # Compute entropy
# Save results
results <- rbind(results, data.frame(File = file, Avg_Identity = round(avg_identity * 100, 2),
Var_Identity = round(var_identity, 4), Avg_Entropy = round(avg_entropy, 4)))
}
# Read aligned sequences
fafsa_data <- readDNAStringSet(file)
sequences <- as.character(fafsa_data)
# Compute identity matrix
identity_matrix <- percentage_identity_matrix(sequences)
# Compute summary statistics
avg_identity <- mean(identity_matrix)
var_identity <- var(as.vector(identity_matrix))
avg_entropy <- alignment_entropy(sequences)  # Compute entropy
# Save results
results <- rbind(results, data.frame(File = file, Avg_Identity = round(avg_identity * 100, 2),
Var_Identity = round(var_identity, 4), Avg_Entropy = round(avg_entropy, 4)))
# Process each aligned FASTA file
for (file in fasta_files) {
cat("Processing:", file, "\n")  # Print progress
# Read aligned sequences
fafsa_data <- readDNAStringSet(file)
sequences <- as.character(fafsa_data)
# Compute identity matrix
identity_matrix <- percentage_identity_matrix(sequences)
# Compute summary statistics
avg_identity <- mean(identity_matrix)
var_identity <- var(as.vector(identity_matrix))
avg_entropy <- alignment_entropy(sequences)  # Compute entropy
# Save results
results <- rbind(results, data.frame(File = file, Avg_Identity = round(avg_identity * 100, 2),
Var_Identity = round(var_identity, 4), Avg_Entropy = round(avg_entropy, 4)))
}
# Save results as CSV
write.csv(results, "identity_summary_V2.csv", row.names = FALSE)
# Print completion message
cat("✅ Analysis complete! Results saved to 'identity_summary.csv'\n")
# Clear workspace
rm(list=ls())
# Load necessary library
library(dplyr)
# Set working directory to where aligned files are stored
setwd("/Users/eesharangani/Desktop/Vibrio_Project/Jan 2025/Keggs/aligned_files")
# Read the original summary file
summary_data <- read.table("summary.txt", header = TRUE, sep = "\t", fill = TRUE, quote = "", stringsAsFactors = FALSE)
# Select only the required columns and remove duplicates
filtered_summary <- summary_data %>%
select(gene_cluster_id, COG20_CATEGORY) %>%
distinct(gene_cluster_id, .keep_all = TRUE)  # Keeps only the first occurrence
# Save the filtered data to a new file
write.table(filtered_summary, "filtered_summary.txt", sep = "\t", row.names = FALSE, quote = FALSE)
# Print completion message
cat("✅ Filtered summary file created: 'filtered_summary.txt'\n")
#Steps to Add in COGS function column:
# Clear workspace
rm(list=ls())
# Step 1: Read identity summary file
identity_summary <- read.csv("identity_summary.csv", stringsAsFactors = FALSE)
# Step 2: Read filtered summary file (contains gene_cluster_id and COG20_CATEGORY)
filtered_summary <- read.table("filtered_summary.txt", header = TRUE, sep = "\t", stringsAsFactors = FALSE)
#Steps to Add in COGS function column:
# Clear workspace
rm(list=ls())
# Step 1: Read identity summary file
identity_summary <- read.csv("identity_summary_V2.csv", stringsAsFactors = FALSE)
# Step 2: Read filtered summary file (contains gene_cluster_id and COG20_CATEGORY)
filtered_summary <- read.table("filtered_summary.txt", header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# Step 2: Read filtered summary file (contains gene_cluster_id and COG20_CATEGORY)
filtered_summary <- read.table("filtered_summary.txt", header = TRUE, sep = "\t", stringsAsFactors = FALSE)
# Step 3: Extract gene_cluster_id from File column in identity_summary
identity_summary <- identity_summary %>%
mutate(gene_cluster_id = sub("_aligned.fasta", "", File))  # Remove `_aligned.fasta` to match IDs
# Step 4: Merge the two datasets based on gene_cluster_id
final_summary <- left_join(identity_summary, filtered_summary, by = "gene_cluster_id")
# Step 5: Save the final dataset
write.csv(final_summary, "final_identity_summary_V2.csv", row.names = FALSE)
# Print completion message
cat("✅ Final merged dataset saved as 'final_identity_summary.csv'\n")
